* TO pour Spark Core RDD
  * reduce(func)
  * ~~broadcast~~
  * accumulator
  * ~~collect()~~
  * ~~collectAsMap()~~
  * count()
  * first()
  * ~~take(n)~~
  * ~~saveAsTextFile(path)/saveAsSequenceFile(path)/saveAsObjectFile(path)~~
  * ~~countByKey()~~
  * ~~map(fun)~~
  * ~~mapToPair(func)~~
  * flatMap(func)
  * flatMapToPair(func)
  * ~~filter(func)~~
  * groupBy()
  * ~~groupByKey()~~
  * ~~reduceByKey(func)~~
  * sortByKey([ascending])
  * ~~join(otherDataset)~~
  * intersect(otherDataset)
  * distinct()
  * union(otherDataset)
  * cogroup
  * ~~mapPartition: bonnes explications sur http://bytepadding.com/big-data/spark/spark-mappartitions/~~
     * ~~Utilisé généralement quant on utilise un service externe~~
  * ~~forEach~~
  * ~~foreachPartition~~
